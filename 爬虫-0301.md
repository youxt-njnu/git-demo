# [第一套](https://github.com/wistbean/learn_python3_spider)

## Chrome高级搜索

完全匹配搜索：""

模糊匹配：*

排除匹配：衣服 -京东

特定网站：衣服 cite:jd.com

特定文件类型：身份证 filetype:xls，还可以搜索PDF书籍

Google以图搜图

inurl, intitle

## chrome的F12

## Fiddler

正确退出：File->Exit

并发测试：选中几个，然后shift+R(要大写)，设置并发测试的次数

[对FIddler的介绍](https://zhuanlan.zhihu.com/p/102392715)

还有两个教程：

https://zhuanlan.zhihu.com/p/341694285

https://zhuanlan.zhihu.com/p/439203346

## urllib、requests

jupyter里的scatch

## 正则表达式、re解析

![image-20230311075559393](D:\programlan\git-demo\images\爬虫-0301\image-20230311075559393.png)

# jupyter里的scatch的最新课件

简单介绍HTTP协议、requests和urllib爬取的小案例；（获取得到页面源代码）

介绍正则表达式和数据解析（re、bs4、xpath），给了些案例；（对获取到的页面源代码进行解析）

request进阶：模拟登录、防盗链、代理；（获取不在页面源代码里的，针对反爬的）

### 爬取网易云评论：

```
# 1）先看看评论是不是在页面源代码里面
# 1.1）在的话，直接提取，然后进行解析，就可以拿到了
# 页面源代码、框架源代码
# 一个网页html里又嵌套了另一个网页html，所以就有两套源代码
# 1.2）不在的话，通过抓包工具，看是哪个url得到的数据：打开network，选中XHR，然后看有哪些请求，点开请求看 Preview，里面有什么内容，和需要的评论有没有关系
# 找到了一个，Request URL: https://music.163.com/weapi/comment/resource/comments/get?csrf_token=dfd994b1434e0deadd85297911546dc5（登陆成功之后，会有csrf_token这个参数的数值，此案例里就没有登录，不需要这个参数了
# 不只是看Headers,也要看Payload
# Payload里的params和encSecKey两个参数，里面的内容看不懂，说明是被加密了的
# 网易会利用encSecKey，把params进行还原，然后实现检索、查询这些
# 此处就需要找到params没加密前是什么样的，是怎么加密的，想办法在程序里模拟加密的过程，手工加密后传递参数，保证程序没有问题



# 几个需求以及解决方式
# 1. 找到未加密的参数                       
# window.arsea(参数, xxxx,xxx,xxx)
# 2. 想办法把参数进行加密(必须参考网易的逻辑), params  => encText, encSecKey => encSecKey
# 3. 请求到网易. 拿到评论信息

# 操作：
# a. 点击initiator,可以看到request call stack,就是请求所调用的栈，反映了发送请求后，js一共执行了哪些脚本和过程，从下往上进行排列，都调用完了，再把请求发送给网易
# b.点击最上面的，看到了经过了压缩后的js代码，点击左下角，将代码进行整理后看到，程序中出现了send的代码
# c.设置断点，之后刷新，到断点处会暂停，之后在右侧可以查看运行到这一步所保存的变量；
# d.打开e0x，里面有request,里面的是cdns，不是需要的get，所以放开继续运行，进入下一次拦截，直到出现get，此时里面的data已经加密了，传入这个函数的参数是e0x，接下来就是看进入这个函数前，e0x有没有被加密
# 使用callback一步步的回溯，看data是否加密,在第五个位置里面运行完之后数据还是未加密的
```

![image-20230324091033027](D:\programlan\git-demo\images\爬虫-0301\image-20230324091033027.png)

![image-20230324091501558](D:\programlan\git-demo\images\爬虫-0301\image-20230324091501558.png)

找到了`window.arsea`,把里面的参数加密了，params=>encText, encSecKey=>encSecKey

![image-20230325100059358](D:\programlan\git-demo\images\爬虫-0301\image-20230325100059358.png)

多线程、多进程、协程，来提高爬虫效率；

进程是资源单位, 每一个进程至少要有一个线程，可以有很多个线程；程序运行的时候，默认会有一个主线程，不推荐开多进程，因为需要开辟内存资源；

线程是执行单位；但开辟也是需要计算机资源的，所以利用线程池可以重复使用线程资源；

协程，使用频率高，通过协程爬取小说和视频；异步

selenium;

scrapy：引擎在中间做调度，首先找到spyder的起始url，在引擎里包装成request对象，然后传给scheduler调度器的队列，然后把request通过引擎找到downloader下载器，对外发送网络请求，然后封装成response对象，返回给引擎，进行解析，送回给spyder。之后传递给pipeline进行数据存储。

![img](D:\programlan\git-demo\images\爬虫-0301\F5B87188FA4D54196481043E94CD1403.png)



# 实战

[爬虫实战（三）----使用百度API获取经纬度/地址（ProgramLan已有相关代码存储）](https://blog.csdn.net/Blank_spaces/article/details/106547571)

[爬虫：多线程与threading模块](https://blog.csdn.net/Blank_spaces/article/details/106547544?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167843429816800197044361%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=167843429816800197044361&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-3-106547544-null-null.blog_rank_default&utm_term=%E7%88%AC%E8%99%AB)

[用Selenium来爬取数据？真挺简单的！](https://cloud.tencent.com/developer/article/1858214)





# 阶段性问题

requests：这么看URL，怎么设置参数获取；

